[
  {
    "objectID": "vignettes/documentation.html",
    "href": "vignettes/documentation.html",
    "title": "l1rotation",
    "section": "",
    "text": "Introduction\nQuick Start\nRefining the Details\n\nfind_local_factors()\ntest_local_factors()"
  },
  {
    "objectID": "vignettes/documentation.html#toc",
    "href": "vignettes/documentation.html#toc",
    "title": "l1rotation",
    "section": "",
    "text": "Introduction\nQuick Start\nRefining the Details\n\nfind_local_factors()\ntest_local_factors()"
  },
  {
    "objectID": "vignettes/documentation.html#intro",
    "href": "vignettes/documentation.html#intro",
    "title": "l1rotation",
    "section": "Introduction",
    "text": "Introduction\n\nlibrary(l1rotation)\n\nThe l1rotation package offers functionality to estimate the loadings matrix in factor models based on the l1-rotation criterion following Freyaldenhoven (2025). The key idea of this estimator is assuming a sparsity pattern in the loadings matrix solves the problem of rotational indeterminacy inherent to factor models.\nFactor models take the form\n\\[\nX = F \\Lambda^{*T} + e\n\\] where\n\n\\(X\\) is a \\(T \\times n\\) data matrix, where there are \\(T\\) rows and \\(n\\) variables, or columns\n\\(F\\) is a \\(T \\times r\\) matrix of \\(r\\) factors that the data is decomposed into\n\\(\\Lambda^{*T}\\) is an \\(r \\times n\\) matrix of true loadings\n\\(e\\) is a \\(T \\times n\\) error matrix.\n\nThis package is designed to identify and estimate \\(\\Lambda^{*}\\) subject to additional sparsity assumptions detailed in Freyaldenhoven (2025)."
  },
  {
    "objectID": "vignettes/documentation.html#start",
    "href": "vignettes/documentation.html#start",
    "title": "l1rotation",
    "section": "Quick start",
    "text": "Quick start\nWe will use the example_data data that ships with the package to show its basic functionality. This data is a matrix containing numeric information with \\(n = 224\\), \\(T = 207\\). In general, data.frames, tibbles, or other data types can also be used with l1rotation functions, as long as all columns are numeric. Note also that the package cannot handle missing values in the data matrix.\nTo start, let’s look at the first seven columns of the example data:\n\nhead(example_data[,1:7])\n#&gt;            V1       V2          V3         V4         V5          V6         V7\n#&gt; [1,] 2.973310 3.563021 -0.04496568 -0.1700720  0.3350881  0.05244918 -1.1922173\n#&gt; [2,] 4.648603 4.817172  0.11200137 -0.5456076  0.1992616  0.33365310 -1.1699467\n#&gt; [3,] 4.162014 3.169177 -1.37955943 -2.5864043 -2.9352645 -0.79856910 -1.7227167\n#&gt; [4,] 2.097454 0.833101 -2.72023404 -2.9061952 -2.7111563 -0.18925022 -1.5973617\n#&gt; [5,] 1.783429 0.364707 -2.24466997 -1.9397028 -0.2521484  1.14563662 -0.6416022\n#&gt; [6,] 5.699166 3.932394  0.46050753 -0.5615002  2.3014500  2.22261949  0.5748873\n\nWe assume that the number of underlying factors can be learned from the data (following the procedure in Bai and Ng (2002)). For the example_data we will use two factors. With just the data, \\(X\\), and the number of factors, \\(r\\), we can start estimating the loadings with local_factors(). This function estimates \\(\\Lambda^{*}\\) and provides helpful diagnostics and figures.\nBelow is an example using example_data (note that estimation can also be run in parallel with a selected number of cores, n_cores):\n\nset.seed(916)\n\nlf &lt;- local_factors(\n  X = example_data, r = 2,\n  parallel = FALSE, n_cores = NULL # Runs non-parallel by default\n)\n\nIn the estimation, local_factors() has reasonable defaults set so the only required arguments are the data you’re decomposing and the number of factors. We use the principal components estimator as the initial estimate of the loadings, \\(\\Lambda_0\\), which can be accessed via the Lambda0 item of the output. The function also computes a quick diagnostic to check whether local factors are present in the data which is given in the output item has_local_factors.\nAdditionally, there are several rotation diagnostics accessible via rotation_diagnostics which give the rotation matrix, \\(R\\) that when multiplied by \\(\\Lambda_0\\) produces \\(\\hat{\\Lambda}^*\\), the value of the l1 norm for each vector, and the frequency that each solution appears in the initial grid of estimates.\n\nlf$rotation_diagnostics\n#&gt; $R\n#&gt;             R          \n#&gt; V1  0.7011144 0.8388980\n#&gt; V2 -0.7130488 0.5442886\n#&gt; \n#&gt; $fval\n#&gt; [1] 145.705 149.850\n#&gt; \n#&gt; $sol_frequency\n#&gt; [1] 259 241\n\nFor a visual interpretation of this rotation, we provide tile plots contrasting the initial estimate \\(\\Lambda_0\\), pc_plot and the rotated estimate, \\(\\hat{\\Lambda}^*\\), rotated_plot.\n\nlf$pc_plot\n\n\n\n\n\n\n\n\nIn the initial principal component estimate, recall that each factor is simply a principal component. Of the 207 variables in \\(X\\), the first factor loads negatively along the variables between 90 and 120, and slightly negatively almost everywhere else. Along the second factor, there are positive loadings between variables 125 and 200 and slightly negative loadings elsewhere. However, it may be difficult to interpret the relationship between variables and factors when all loadings are nonzero.\n\nlf$rotated_plot\n\n\n\n\n\n\n\n\nThe second estimate is a rotated version of the principal components estimator optimized for sparsity. It will rotate each vector of the principal components loadings matrix until it finds a set of factors that are as sparse as possible. As we can see, the second plot may provide a stronger interpretation as variables 90-200 load negatively on the first factor while variables 0-120 load negatively on the second factor. The loadings along the other variables are close to zero, suggesting no relationship. With this rotation, we can more easily identify which sets of variables are related to which latent factors due to the revealed sparsity pattern."
  },
  {
    "objectID": "vignettes/documentation.html#details",
    "href": "vignettes/documentation.html#details",
    "title": "l1rotation",
    "section": "Refining the details",
    "text": "Refining the details\nl1rotation supplies two additional functions, find_local_factors() and test_local_factors() which provide additional functionality to support the main local_factors() function.\n\nfind_local_factors()\nThis function has an additional argument, Lambda0, that allows the user to specify any orthonormal basis of the loadings rather than defaulting to the principal component estimator. Here are some plausible alternative initial estimates:\n\nMLE estimation\nSparse Orthogonal Factor Regression (SOFAR) as in Uematsu et al. (2019).\n\n\n\ntest_local_factors()\nThis function tests for the presence of local factors given an estimate of the loadings matrix, Lambda. Since the results in this package rely upon a sparsity pattern in the true loadings matrix, test_local_factors() provides a simple diagnostic to check this assumption.\nFor a given loading matrix estimate, \\(\\hat{\\Lambda}\\), we can consider maximizing across factors the number of individual loadings in \\(\\hat{\\Lambda}\\) smaller than some threshold \\(h_n\\). This gives us an idea of how many “small” loadings there are in \\(\\hat{\\Lambda}\\):\n\\[\n\\mathcal{L(\\hat{\\Lambda})} = \\max_k\\left(\\sum_{i=1}^n 1\\{\\hat{|\\lambda}_{ik}| &lt; h_n \\}\\right)\n\\] With this number, we can then check whether the number of “small” loadings is larger than \\(\\gamma n\\)\n\\[\n\\text{has_local_factors} = 1\\{\\mathcal{L}(\\hat{\\Lambda})  \\geq \\gamma n \\}.\n\\]\nReturning to our lf results, we can take a look at the value of has_local_factors.\n\nlf$has_local_factors\n#&gt; [1] TRUE\n\nThis value is the result of test_local_factors(). To verify, we can call test_local_factors() on two different estimates: the principal components estimate, Lambda0, and the l1rotation estimate, Lambda.\n\n# Check for local factors in PC estimate...\ntest_pc_estimate &lt;- test_local_factors(X = example_data, r = 2, loadings = lf$initial_loadings)\n\n# And rotated estimate\ntest_rot_estimate &lt;- test_local_factors(X = example_data, r = 2, loadings = lf$rotated_loadings)\n\ntest_pc_estimate$has_local_factors\n#&gt; [1] FALSE\ntest_rot_estimate$has_local_factors\n#&gt; [1] TRUE\n\nThis confirms that no local factors are detected in the principal components estimate (no sparsity pattern is observed), and that local factors are present in the rotated estimate.\n\nNote that rotating the initial estimate, optimizing for sparsity, is not guaranteed to produce a sparse loadings matrix if the true loadings matrix itself is not sparse. However, if a sparsity pattern does exist in the true loadings matrix, this procedure will recover it.\n\n\nround_hn &lt;- round(test_rot_estimate$h_n, digits = 3)\n\nlf$small_loadings_plot + \n  ggplot2::labs(\n    title = 'Number of \"small\" loadings per factor', \n    caption = paste('\"Small\" is defined as loadings less than', round_hn)\n  )\n\n\n\n\n\n\n\n\nFinally, given the testing plot above, we can conclude that factors 1 and 2 are likely local since they have enough “small” loadings (i.e., smaller than a value of \\(h_n = 1/\\log(n) =\\) 0.188 in the example_data)."
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "l1rotation",
    "section": "Overview",
    "text": "Overview\nThe l1rotation package implements the l1-rotation rotation criterion to identify and estimate the loadings matrix in factor models following Freyaldenhoven (2025)."
  },
  {
    "objectID": "index.html#installation",
    "href": "index.html#installation",
    "title": "l1rotation",
    "section": "Installation",
    "text": "Installation\n\n# Install from CRAN\ninstall.packages(\"l1rotation\")\n\n# Install latest version from GitHub\ninstall.packages(\"devtools\")\ndevtools::install_github(\"SimonFreyaldenhoven/l1rotation\")"
  },
  {
    "objectID": "index.html#usage",
    "href": "index.html#usage",
    "title": "l1rotation",
    "section": "Usage",
    "text": "Usage\nFind a minimal example below. For more examples see the package documentation and vignette.\n\n\nlibrary(l1rotation)\nset.seed(916) \n\n# Minimal example with 4 factors, where X is a 224 by 207 matrix\nlf &lt;- local_factors(X = example_data, r = 2)\n\n# Rerun with parallel processing\nlf_lambdas &lt;- find_local_factors(X = example_data, r = 2, parallel = TRUE, n_cores = 10)\n\n# Visualize Principal Component estimate of the loadings\nlf$pc_plot\n\n\n\n\n\n\n\n\n# Visualize l1-rotation loadings\nlf$rotated_plot"
  },
  {
    "objectID": "index.html#citation",
    "href": "index.html#citation",
    "title": "l1rotation",
    "section": "Citation",
    "text": "Citation\nSimon Freyaldenhoven.”Identification Through Sparsity in Factor Models: the l1-rotation criterion” Working Paper, February 2025.\nSimon Freyaldenhoven, Ryan Kobler. “l1rotation package.” Code and data repository at https://github.com/SimonFreyaldenhoven/l1rotation, March 2025."
  },
  {
    "objectID": "intuition.html",
    "href": "intuition.html",
    "title": "Background and Intuition",
    "section": "",
    "text": "Factor models seek to distill often high dimensional data into a set of underlying or latent factors that capture much of the original variation in the data.\nWe say the data, \\(X\\), follows a factor structure if:\n\\[\n\\begin{align}\n\\underset{(n \\times 1)}{X_{t}} &= \\underset{(n \\times r)}{\\Lambda^{*}_{\\vphantom{t}}} \\underset{(r \\times 1)}{F_t} + \\underset{(n \\times 1)}{e_{t}} \\forall t, \\qquad \\text{or more compactly,} \\qquad \\underset{(T \\times n)}{X} = \\underset{(T \\times r)}{F} \\underset{(r \\times n)}{\\Lambda^{*'}}  + \\underset{(T \\times n)}{e}\n\\end{align}\n\\]\nwhere there are\n\n\\(T\\) rows (observations),\n\\(n\\) columns (variables), and\n\\(r\\) factors. And we can refer to\n\\(\\lambda_{ik}\\) as the entry in \\(\\Lambda^{*T}\\) in the \\(i\\)th row and \\(k\\)th column\n\nThis \\(\\lambda_{ik}\\) shows how factor \\(k\\) is related to, or “loads onto,” variable \\(i\\).\nNote that \\(r\\) can be learned from the data (cite) so we can assume that \\(r\\) is known. Next, we’re interested in estimating a set of factors \\(F\\), that compress the data into fewer columns, and loadings \\(\\Lambda^*\\), that show how the factors are related to the original columns. However, there is no unique \\(F\\) and \\(\\Lambda^*\\) that satisfies the equation above. In fact, infinitely many will. This is referred to as rotational indeterminacy and it can pose issues in the interpretability of the loadings matrix."
  },
  {
    "objectID": "intuition.html#factor-models-primer",
    "href": "intuition.html#factor-models-primer",
    "title": "Background and Intuition",
    "section": "",
    "text": "Factor models seek to distill often high dimensional data into a set of underlying or latent factors that capture much of the original variation in the data.\nWe say the data, \\(X\\), follows a factor structure if:\n\\[\n\\begin{align}\n\\underset{(n \\times 1)}{X_{t}} &= \\underset{(n \\times r)}{\\Lambda^{*}_{\\vphantom{t}}} \\underset{(r \\times 1)}{F_t} + \\underset{(n \\times 1)}{e_{t}} \\forall t, \\qquad \\text{or more compactly,} \\qquad \\underset{(T \\times n)}{X} = \\underset{(T \\times r)}{F} \\underset{(r \\times n)}{\\Lambda^{*'}}  + \\underset{(T \\times n)}{e}\n\\end{align}\n\\]\nwhere there are\n\n\\(T\\) rows (observations),\n\\(n\\) columns (variables), and\n\\(r\\) factors. And we can refer to\n\\(\\lambda_{ik}\\) as the entry in \\(\\Lambda^{*T}\\) in the \\(i\\)th row and \\(k\\)th column\n\nThis \\(\\lambda_{ik}\\) shows how factor \\(k\\) is related to, or “loads onto,” variable \\(i\\).\nNote that \\(r\\) can be learned from the data (cite) so we can assume that \\(r\\) is known. Next, we’re interested in estimating a set of factors \\(F\\), that compress the data into fewer columns, and loadings \\(\\Lambda^*\\), that show how the factors are related to the original columns. However, there is no unique \\(F\\) and \\(\\Lambda^*\\) that satisfies the equation above. In fact, infinitely many will. This is referred to as rotational indeterminacy and it can pose issues in the interpretability of the loadings matrix."
  },
  {
    "objectID": "intuition.html#problem-rotational-indeterminacy",
    "href": "intuition.html#problem-rotational-indeterminacy",
    "title": "Background and Intuition",
    "section": "Problem: Rotational Indeterminacy",
    "text": "Problem: Rotational Indeterminacy\nTo see the problem, let \\(H\\) be any nonsingular \\(r \\times r\\) matrix. We can define \\(\\Lambda^0 = \\Lambda^* (H^T)^{-1}\\) and \\(F^0 = FH\\). This tells us that\n\\[\n\\begin{align}\nX & = F^0 \\Lambda^{0'} \\\\\n& = FH (H^{-1'})' \\Lambda^{*'}\\\\\n& = FH H^{-1} \\Lambda^{*'} \\\\\n& = F \\Lambda^{*'}\n\\end{align}\n\\]\nHence \\(X\\) can be explained identically well by any “rotation” \\(H\\) of the loadings and factors, and each rotation provides a different interpretation of how the factors and variables are related. So how can we find the correct rotation, or interpretation?"
  },
  {
    "objectID": "intuition.html#solution-use-sparsity",
    "href": "intuition.html#solution-use-sparsity",
    "title": "Background and Intuition",
    "section": "Solution: Use Sparsity",
    "text": "Solution: Use Sparsity\nThe key idea in Freyaldenhoven (2025) is that assuming a sparsity pattern in the true loadings matrix \\(\\Lambda^*\\) solves the issue of rotational indeterminacy since the sparsity pattern is not invariant to rotations. Intuitively, any rotation (or linear combination) of a sparse loading vector will be less sparse. In general, with PCA we can obtain estimates that are a linear combination of the true loadings vectors. That is, we can obtain principal component vectors in the columns of \\(\\Lambda^0\\) such that\n\\[\n\\Lambda^0 = \\Lambda^*(H^{-1})' + \\epsilon\n\\] where \\(H\\) is as above.\nTherefore, since \\(\\Lambda^0\\) is a linear combination of \\(\\Lambda^*\\), \\(\\Lambda^*\\) is also equal to some linear combination of \\(\\Lambda^0\\). And because linear combinations of sparse loadings are generally dense, there must be a linear combination of \\(\\Lambda^0\\) that is sparse. We can then use this sparse linear combination of \\(\\Lambda^0\\) as an estimator for \\(\\Lambda^*\\).\nAssuming the true loadings are sparse is fairly reasonable, particularly when we’re interested in factors that are thought to affect only a subset of the original columns (i.e., local factors). Such factors are common in economic applications."
  },
  {
    "objectID": "intuition.html#recovering-sparsity",
    "href": "intuition.html#recovering-sparsity",
    "title": "Background and Intuition",
    "section": "Recovering Sparsity",
    "text": "Recovering Sparsity\nNow, how do we find this sparse linear combination of loading vectors?\nThe steps are below:\n\nTake the principal components estimator as starting point to obtain \\(\\Lambda^0\\)\nFind loadings \\(\\Lambda^*\\) equal to the rotation of \\(\\Lambda^0\\) that minimizes the number of non-zero elements (the \\(l_0\\) norm) in the loading vectors\n\nUnfortunately, the \\(l_0\\) norm is infeasible to optimize over in practice. To see why, let’s examine the figure below. The distance from the origin to each red point depicts the \\(l_0\\) norm of points along the unit circle. The blue points depict the \\(l_1\\) norm and the gray points depict the \\(l_2\\) norm In the \\(l_2\\) case, these distances remain constant as we traverse the circle.\n\n\n\nGeometric Intuition of Various Norms\n\n\nThe \\(l_0\\) norm directly computes the number of nonzero elements, shown by the red discontinuities at (1,0), (0,1), (-1,0) and (0, -1). From the blue dots, we can see that the \\(l_1\\) norm is also minimized at the same points as the \\(l_0\\) norm. But the \\(l_1\\) norm has a much smoother descent to the above four points compared with the discontinuities that occur with the red points. This smoother function is much easier to traverse and optimize over.\nSo throughout, we’ll continue to maximize sparsity indirectly, but more easily by swapping the \\(l_1\\) norm for the \\(l_0\\) norm for the optimization benefits.\n\nObjective Function\nFormally, using the \\(l_1\\) norm, our objective function becomes\n$$ _R ||^0R||\n$$ such that \\(R\\) is nonsingular and \\(||r_k||_2 = 1\\) where \\(r_k\\) is a row in \\(R\\). This ensures we look only at rotations of each initial loading vector, keeping the length of each loading vector fixed.\nNote that the above objective function can be computed and optimized separately for each loading vector individually."
  },
  {
    "objectID": "intuition.html#a-simple-example",
    "href": "intuition.html#a-simple-example",
    "title": "Background and Intuition",
    "section": "A simple example",
    "text": "A simple example\nLet’s consider an example we can visualize fully: we’ll use the following simulated data that has three columns (\\(n = 3\\)) and suppose we know that there are two factors, \\(r = 2\\). Below only the first 6 of 224 total rows are printed.\n\n\n          V1         V2         V3\n1  0.6624031 -0.1829747 -0.5026830\n2  0.3709347  1.2174531 -1.1426082\n3 -1.2000343 -1.2247438 -1.6778625\n4  0.5225106  0.9187241 -0.5469126\n5  0.4667395 -0.8284538 -0.5829659\n6 -0.2022476 -0.1169972  0.1865991\n\n\nLet the true loadings matrix, \\(\\Lambda^*\\) be a matrix with 2 columns and 3 rows. As we can see there is a sparsity pattern in the matrix with the first factor affecting only the first column and the second factor affecting the second and third columns of \\(X\\).\n\n\n  loadings_1 loadings_2\n1   1.024946  0.0000000\n2   0.000000  1.3517553\n3   0.000000  0.2765392\n\n\nNow, let’s look at the PCA estimator for \\(X\\) for the first two principal components. As we can see the zeroes from above are no longer present, but the principal component estimate still provides a great starting point as they are generally linear combinations of the true loading vectors as \\(n \\to \\infty\\) (cite).\n\n\n  loadings_1 loadings_2\n1 -0.9331464 -1.4110908\n2 -1.3219055  0.6307241\n3 -0.6179026  0.7816712\n\n\nLet’s try to get a clearer picture of what we have so far by visualizing these components. Below is an interactive visualization of\n\nthe data points, \\(X\\)\nthe plane spanned by the principal component vectors (blue plane)\nthe principal component vectors \\(\\Lambda^0\\) (orange)\nthe true loading vectors \\(\\Lambda^*\\) (blue)\n\n\nInteractive Visualization\nAs we can see, the orange principal component vectors span the blue plane. By definition, these two vectors provide the directions of maximal variance of the data. We can also imagine this data cloud projected onto the plane to give us a sense of how the data could be compressed from 3 to 2 dimensions. Notice that the orange vectors are nowhere near the true loading vectors shown in blue. However, simply rotating these orange vectors would give us solutions that are closer to the true (blue) vectors.\nUsing the sliders below which control the angle of rotation for each vector separately, try rotating the orange vectors along the blue plane to find a rotation that lines up as close as possible to the blue vectors.\nSince \\(n = 3\\) is small, we can see there is some noise in the PC estimate, resulting in the loading vectors being located slightly off of this plane. Still, we can get a better estimate of the true vectors by rotating.\n\nmath = require('https://cdnjs.cloudflare.com/ajax/libs/mathjs/1.5.2/math.min.js')\nd3 = require(\"d3@3\")\n//functionPlot = require(\"https://unpkg.com/function-plot@1/dist/function-plot.js\")\n\nTHREE = {\n  const THREE = window.THREE = await require(\"three@0.130.0/build/three.min.js\");\n  await require(\"three@0.130.0/examples/js/controls/OrbitControls.js\").catch(() =&gt; {});\n  return THREE;\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrenderer.domElement\n\n\n\n\n\n\nAs we try different angles of rotation out we can see their impact on the objective function we’re optimizing over. The rotation that best aligns with the true loadings vectors should also occur at the local minima of the objective function.\nviewof theta_value = Inputs.range(\n  [-3.13, 3.13], \n  {value: 1.57, step: .01, label: \"Vector 1 angle (theta):\"}\n)\n\nviewof theta_value2 = Inputs.range(\n  [-3.13, 3.13], \n  {value: 3.13, step: .01, label: \"Vector 2 angle (theta):\"}\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhighlighted_points = obj_data.filter(function(penguin) {\n  return (theta_value - 0.005 &lt; penguin.theta && theta_value + 0.005 &gt; penguin.theta) ||\n         (theta_value2 - 0.005 &lt; penguin.theta && theta_value2 + 0.005 &gt; penguin.theta)\n})\n\nobj_data = FileAttachment(\"data/obj_function_data.csv\").csv({typed: true});\n\nPlot.plot({\n  height: 100,\n  width: 700,\n  grid: true,\n  marks: [\n    Plot.dot(obj_data, {\n      x: \"theta\", \n      y: \"y\",\n      r: 1\n    }),\n    Plot.dot(highlighted_points, {\n      x: \"theta\", \n      y: \"y\", \n      fill: \"orange\", \n      r: 5\n    })\n  ],\n  y: {\n    label: \"\"\n  },\n  title: \"Objective function\",\n  style: {\n    background: \"transparent\",\n    fontSize: 12\n  }\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nheight = 500;\ncamera = {\n  const fov = 45;\n  const aspect = width / height;\n  const near = 1;\n  const far = 1000;\n  const camera = new THREE.PerspectiveCamera(fov, aspect, near, far);\n  camera.position.set(2, 2, -2)\n  camera.lookAt(new THREE.Vector3(0, 0, 0));\n  return camera;\n}\naxesHelper = {const axesHelper = new THREE.AxesHelper( 5 );\nreturn axesHelper;\n}\n\ngridHelper = {const size = 10;\nconst divisions = 10;\n\nconst gridHelper = new THREE.GridHelper( size, divisions );\nreturn gridHelper;\n}\n\nrenderer = {\n  const renderer = new THREE.WebGLRenderer({antialias: true});\n  renderer.setSize(width, height);\n  renderer.setPixelRatio(devicePixelRatio);\n  const controls = new THREE.OrbitControls(camera, renderer.domElement);\n  controls.addEventListener(\"change\", () =&gt; renderer.render(scene, camera));\n  invalidation.then(() =&gt; (controls.dispose(), renderer.dispose()));\n  return renderer;\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npca_vectors = await FileAttachment('data/lambda0.csv').csv({typed: true})\ntrue_vectors = await FileAttachment('data/truth_normal.csv').csv({typed: true})\n\n\ndisplay_arrow = {const dir = new THREE.Vector3( pca_vectors[0].V1, pca_vectors[1].V1, pca_vectors[2].V1);\n\n//normalize the direction vector (convert to vector of length 1)\ndir.normalize();\n\nconst origin = new THREE.Vector3( 0, 0, 0 );\nconst length = 1;\nconst hex = 0xFFA500;\n\nconst arrowHelper = new THREE.ArrowHelper( dir, origin, length, hex );\nreturn arrowHelper;\n        }\n        \ndisplay_arrow2 = {const dir = new THREE.Vector3( pca_vectors[0].V2, pca_vectors[1].V2, pca_vectors[2].V2);\n\n//normalize the direction vector (convert to vector of length 1)\ndir.normalize();\n\nconst origin = new THREE.Vector3( 0, 0, 0 );\nconst length = 1;\nconst hex = 0xFFA500;\n\nconst arrowHelper = new THREE.ArrowHelper( dir, origin, length, hex );\nreturn arrowHelper;\n        }\n        \n        \ntrue_vector1 = {const dir = new THREE.Vector3( true_vectors[0].V1, true_vectors[1].V1, true_vectors[2].V1);\n\n//normalize the direction vector (convert to vector of length 1)\ndir.normalize();\n\nconst origin = new THREE.Vector3( 0, 0, 0 );\nconst length = 1;\nconst hex = 0x0000FF;\n\nconst arrowHelper = new THREE.ArrowHelper( dir, origin, length, hex );\nreturn arrowHelper;\n        }\n        \ntrue_vector2 = {const dir = new THREE.Vector3( true_vectors[0].V2, true_vectors[1].V2, true_vectors[2].V2);\n\n//normalize the direction vector (convert to vector of length 1)\ndir.normalize();\n\nconst origin = new THREE.Vector3( 0, 0, 0 );\nconst length = 1;\nconst hex = 0x0000FF;\n\nconst arrowHelper = new THREE.ArrowHelper( dir, origin, length, hex );\nreturn arrowHelper;\n        }\n        \n        \n        \nmath_plane = {\n  const v1 = new THREE.Vector3( pca_vectors[0].V1, pca_vectors[1].V1, pca_vectors[2].V1 )\n  const v2 = new THREE.Vector3( pca_vectors[0].V2, pca_vectors[1].V2, pca_vectors[2].V2 )\n  const normal = new THREE.Vector3().crossVectors(v1, v2).normalize();\n  const plane = new THREE.Plane(normal);\n  const helper = new THREE.PlaneHelper( plane, 2, 0x0000FF );\n  return helper;\n  \n}       \n\n\n\npoints = {\n  const data = await FileAttachment('data/data.csv').csv({typed: true});\n  const geometry = new THREE.BufferGeometry();\n  const positions = new Float32Array(data.flatMap(d =&gt; [d.V1, d.V2, d.V3]));\n  geometry.setAttribute(\"position\", new THREE.BufferAttribute(positions, 3));\n\n  const material = new THREE.PointsMaterial({color: 0x000000, size: 0.1});\n  const points = new THREE.Points(geometry, material);\n  return points\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nscene = {\n  const scene = new THREE.Scene();\n  scene.background = new THREE.Color(0xffffff);\n  scene.add(gridHelper);\n  scene.add(axesHelper);\n  scene.add(display_arrow);\n  scene.add(display_arrow2);\n  scene.add(true_vector1);\n  scene.add(true_vector2);\n  scene.add(math_plane);\n  scene.add(points);\n  return scene;\n}\n\n{const normal = math_plane.plane.normal;\nconst rotation_matrix = new THREE.Matrix4().makeRotationAxis(normal, theta_value);\nconst new_direction = new THREE.Vector3(pca_vectors[0].V1, pca_vectors[1].V1, pca_vectors[2].V1).normalize().applyMatrix4(rotation_matrix);\ndisplay_arrow.setDirection(new_direction);             \nrenderer.render(scene, camera);\nyield null;\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{const normal = math_plane.plane.normal;\nconst rotation_matrix = new THREE.Matrix4().makeRotationAxis(normal, theta_value2);\nconst new_direction = new THREE.Vector3(pca_vectors[0].V2, pca_vectors[1].V2, pca_vectors[2].V2).normalize().applyMatrix4(rotation_matrix);\ndisplay_arrow2.setDirection(new_direction);             \nrenderer.render(scene, camera);\nyield null;\n\n}\n\n\n\n\n\n\n\n\nThe above visualizations show the relationship between the angles of rotation, the objective function optimizing for sparsity, and the position of the vectors in space relative to the true loadings. This is the crux of the intuition and reasoning behind this research and accompanying package."
  }
]